{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czxW4C_gf5Eo"
   },
   "source": [
    "# DATASCI 290 - GenAI - Assignment 5\n",
    "\n",
    "In Assignment 5 you will create and test a RAG system yourself, and write a corresponding business proposal.\n",
    "\n",
    "The overall scenario is as follows:\n",
    "\n",
    "You work at a tech company that is looking for new ways to organize their question answering and search capabilities to accelerate both engineering activity and the marketing team's production. The company also wants to roll out new GenAI-based products, so a lot of the questions will center around Generative AI concepts. The company has about 300 engineers and a marketing staff of 40. Product releases are done quarterly.\n",
    "\n",
    "Your role is to implement and conduct a (mini-)POC helping the company to evaluate RAG capabilities for the improvement of their document search (and corresponding question answering), supporting particularly the engineering and marketing organizations. You will have a gold dataset with 'good' responses to questions from marketing and engineering teams. You need to develop metric(s) that help you to evaluate how well your RAG system performs relative to the gold data. You should work with the tunables of the setup (LLM, chunking, embeddings, ...) for your iterations.\n",
    "\n",
    "You will also need to write up your findings as a short proposal.\n",
    "\n",
    "(See instructions throughout this notebook.)\n",
    "\n",
    "So overall, the goals of this assignment is for you to:\n",
    "\n",
    "*  To implement a RAG system using LangChain\n",
    "*  Be able to formulate metric(s) that you may want to choose as your evaluation to what degree your system replicates gold answers (labeled data) that we will provide.\n",
    "* Try out various hyper-parameters and settings to see which configuration works the best (given your chosen metric)  \n",
    "* Write a comprehensive evaluation, which also includes risks and limitations (and a lot more)\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "1. Set-Up\n",
    "\n",
    "2. Base RAG components\n",
    "\n",
    "    We will provide a base LangChain-based framework for you to use for your RAG system. The components weâ€™ll need include:  \n",
    "\n",
    "  2.1 Text Embeddings    \n",
    "  2.2 Text Chunking   \n",
    "  2.3 The Vector DB & Semantic Search  \n",
    "  2.4 The Language Model   \n",
    "  2.5 Testing the LLM in a LangChain Chain   \n",
    "  2.6. Setting up a simple RAG Chain     \n",
    "\n",
    "\n",
    "3. Using RAG  \n",
    "  3.1 Loading of Data  \n",
    "  3.2 Test Queries\n",
    "\n",
    "\n",
    "4.  Evaluations\n",
    "\n",
    "  Here, you will conduct your evaluations\n",
    "\n",
    "\n",
    "5. Final Results\n",
    "\n",
    "  In this section you provide the RAG answers to the test questions\n",
    "\n",
    "RULES:  \n",
    "\n",
    "* You can only use the language models specified here  \n",
    "* You can only use the embedding methods we discuss  \n",
    "* You can only use the focuments we provide. And they all must be in your store   \n",
    "* Apart from the provided specifications, some of the things you can freely experiment with include chunk sizes, prompts, etc.\n",
    "\n",
    "\n",
    "**To run this notebook** you should copy it to your personal Colab Pro Google account by uploading it into your Google Drive. From there you can open it as a Colab notebook and run it.  Note it needs a T4 GPU to run.  You may be able to run it in a free Colab notebook.\n",
    "\n",
    "NOTES:\n",
    "* The Open Source Model is not trained for safety. So unsafe answers could be returned.\n",
    "\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We will first install a number of libraries and import what we will need.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIlhfQj-KUlZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install git+https://github.com/huggingface/transformers\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -q install bitsandbytes accelerate\n",
    "!pip -q install langchain\n",
    "!pip install einops\n",
    "!pip install faiss-gpu\n",
    "!pip install langchain_community\n",
    "!pip install --upgrade --quiet chromadb bs4 qdrant-client\n",
    "!pip install langchainhub\n",
    "!pip install -U langchain-huggingface\n",
    "!pip install -U langchain-cohere\n",
    "!pip install --upgrade --quiet  wikipedia\n",
    "!pip install --upgrade --quiet  arxiv\n",
    "!pip install --upgrade --quiet  pymupdf\n",
    "\n",
    "!pip install xmltodict\n",
    "\n",
    "!pip install cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NjcvYABKieZ",
    "outputId": "eeabb83c-d655-47ec-cde4-ca5240d9a97c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import locale\n",
    "\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PubMedLoader\n",
    "\n",
    "#from langchain_community.chat_models import ChatCohere\n",
    "\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faZ5fLk_xxAO"
   },
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stlb_ciPxxWA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJWr9TkCa7gG"
   },
   "source": [
    "Add your keys from the secret store (do **NOT** print them out or leave them exposed as plaintext in your notebook!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll9IqkVMa7qP"
   },
   "outputs": [],
   "source": [
    "COHERE_API_KEY = userdata.get('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlSHHPW-f3ZL"
   },
   "source": [
    "## 2. Building the Components of our RAG System\n",
    "\n",
    "Let us introduce and test the base components of our RAG system. We will largely use the Hugging Face and LangChan libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N3fqR5vKV9b"
   },
   "source": [
    "### 2.1 The Embedding Model\n",
    "\n",
    "We will need to represent text (pieces) as vectors. For this, we will use the [sentence_transformer]() architecture.\n",
    "\n",
    "\n",
    "\n",
    "**NOTE:** The models you can use are: 'all-mpnet-base-v2', 'all-MiniLM-L6-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', and 'avsolatorio/GIST-Embedding-v0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_AqjidjKWif"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cgzrqje8PN8S",
    "outputId": "58c29fd8-be5a-457e-b4a5-9ecb218d1357"
   },
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = base_embeddings.embed_query(text)\n",
    "print(f'Embedding dimension: {len(query_result)}')\n",
    "\n",
    "doc_result = base_embeddings.embed_documents([\"Germany won the World Cup 4 times.\", \"This is not a test document.\"])\n",
    "len(doc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPQGVSGgzG6F"
   },
   "source": [
    "Do those dimensions look correct?\n",
    "\n",
    "Now lets see if the embedding model is working as we want.  Ideally our embeddings go beyond shared words and capture the underlying meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKlCdEeKveqN",
    "outputId": "e278c6f1-eac0-44da-da8a-0261c57a0645"
   },
   "outputs": [],
   "source": [
    "#Let's see how well our embeddng model works\n",
    "similarity = cosine_similarity([query_result], doc_result)[0]\n",
    "\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAp4fF5dPhbM"
   },
   "source": [
    "That's how you should define your embedding models.\n",
    "\n",
    "Next, we turn to text chunks.\n",
    "\n",
    "### 2.2. Loading and Chunking Texts\n",
    "\n",
    "We first need to load the documents. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ak5XlaUP1cW"
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZXtDqKcQHlT"
   },
   "source": [
    "We will need to split the  text in chunks that are 'suitable' as retrieval units. Let's for starters define a chunk size of 128 and have no overlap between the chunks:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhcWoTajQnw6",
    "outputId": "a1adfda5-1a1f-470c-a3ad-da23d0b394d2"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print('Number of splits/chunks: ', str(len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUrNISkVRB_U"
   },
   "source": [
    "Ok, so it looks like we have now many splits (chunks) from one document. Here is how you can get the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "du9avBkhP1ll",
    "outputId": "cb339735-e300-4494-e00c-cb042e87a35f"
   },
   "outputs": [],
   "source": [
    "splits[39].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-RuP_9xRVtG"
   },
   "source": [
    "Perfect. Now we have the splits and embeddings. Next, the embeddings need to be stored in a vector db.\n",
    "\n",
    "### 2.3 Storing the Embeddings of Chunks in Vectorstores\n",
    "\n",
    "After loading and chunking the data, we need to save the vector representations of the chunks in a vectorstore. We will use Qdrant here for simplicity. We load the splits (structured chunks) and the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T3FVDoJRglN"
   },
   "outputs": [],
   "source": [
    "vectorstore = Qdrant.from_documents(splits,\n",
    "    base_embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"test\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar4bpzJtRrio"
   },
   "source": [
    "The nice thing is that the vector store also does the similarity searches for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiSqastIP1oT"
   },
   "outputs": [],
   "source": [
    "query = \"What is Chain of Thought doing?\"\n",
    "docs = vectorstore.similarity_search_by_vector(base_embeddings.embed_query(query)) # will rank the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7CO0lvSR0MA",
    "outputId": "5ac54ef3-695f-417e-f119-52d80f897315"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyypf-VZRsxg"
   },
   "source": [
    "Looks good! We have an ordered list of documents that seem to relate to the question. That is what we need.\n",
    "\n",
    "The last major component is the actual LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDDom9EbKXCX"
   },
   "source": [
    "### 2.4. The LLM\n",
    "\n",
    "We will use one Open Source Model (\"mistralai/Mistral-7B-Instruct-v0.2\") and one Proprietery Model (Cohere) for our tests. Let's first set up the OS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YooxnCPNOoQ7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         )\n",
    "\n",
    "\n",
    "llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVGOr2cV_q6I"
   },
   "source": [
    "We use the model first to generate a Hugging Face pipeline. A pipeline simplifies the process of actually generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rvxo5OKwvjNN",
    "outputId": "a89fe24f-05e5-4cc2-b74a-53469f926d88"
   },
   "outputs": [],
   "source": [
    "mistral_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_mistral_model,\n",
    "    tokenizer=llm_mistral_tokenizer,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id\n",
    "\n",
    "    # wrapping the Hugging Face pipeline into a LangChain object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOHYPzDiTRIK"
   },
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAUHEvq_TRSv",
    "outputId": "8e213f18-142c-405a-976c-b1adcac6153b"
   },
   "outputs": [],
   "source": [
    "mistral_pipe(\"[INST]Give me a two-sentence story about an apple![/INST]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh-60AEBTODZ"
   },
   "source": [
    "Reasonable!\n",
    "\n",
    "We will also use a Cohere model, but will create this below as part of the LangChain framework.\n",
    "\n",
    "### 2.5 Testing the LLM in a LangChain Chain\n",
    "\n",
    "Chains will be defined and discussed in Week 11. In short, they are convenient programmatic ways to deal with 'chains' of actions that involve LLMs. For example, a list of events like 'here is a city name. Plug that city name into prompt template, then generate a story about that city. Lastly, format the model output as a string' can be easily handled by LangChain's Chain framework. In this case, the Chain would consist of the prompt template, the LLM, and the String Formatter. The parameter (the city in this case) will be provided at run time by invocation of the Chain. Let's test that.\n",
    "\n",
    "To use a Hugging Face model in a LangChain environment, we need to wrap the model into a LangChain pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1LSyyFmTOYP",
    "outputId": "82cb0449-4eeb-49b2-8cc9-61d4ec867e90"
   },
   "outputs": [],
   "source": [
    "mistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FkQde8yZYqZ"
   },
   "source": [
    "Next, we need to define a template and create a corresponding prompt template that can take any questiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh3R9445K5ct"
   },
   "outputs": [],
   "source": [
    "test_llm_template = \"\"\"[INST] Give me a two-sentence story about an {object}! [/INST]\"\"\"\n",
    "test_llm_prompt_template = PromptTemplate(template=test_llm_template, input_variables=[\"object\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V6PO2LISDbT"
   },
   "source": [
    "Let's define a Chain, a static flow of actions that (usually) involve at least a definition of the variables used in the chain, one or more templates, LLM step(s) and potentially other actions. This would be a chain that declares the variable 'object' to be expected when the chain is invoked, then inserts it into the template, and passes this to our mistral model pipeline (wrapped as a LangChain object):    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSvSoD7hZb4Z"
   },
   "outputs": [],
   "source": [
    "test_llm_chain_short = (\n",
    "    {\"object\": RunnablePassthrough()}\n",
    "    | test_llm_prompt_template\n",
    "    | mistral_llm_lc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "jH8lUWWSZ-9n",
    "outputId": "0779e4c7-912d-42f0-b524-4bd22dfd73e5"
   },
   "outputs": [],
   "source": [
    "test_llm_chain_short.invoke('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYeKmOoMaSKS"
   },
   "source": [
    "Works too. We will use this notation moving forward.\n",
    "\n",
    "Next, how would we do this with a Cohere Chat Model instead of Mistral?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JROGul-gZcAU"
   },
   "outputs": [],
   "source": [
    "cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h3DW_bBfPsI"
   },
   "source": [
    "This can be plugged straight into the Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ5bzWTVaZdO"
   },
   "outputs": [],
   "source": [
    "test_cohere_llm_chain_short = (\n",
    "    {\"object\": RunnablePassthrough()}\n",
    "    | test_llm_prompt_template\n",
    "    | cohere_chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZldFGowOciCk",
    "outputId": "19e3d8c9-0a45-4a98-883b-0f11657cf43a"
   },
   "outputs": [],
   "source": [
    "test_cohere_llm_chain_short.invoke('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CasSDdMnF8z"
   },
   "source": [
    "Works! (Note: you may want to review the format of the template. The one we used here is the one from Mistral, and the format may or may not be optimal for Cohere.)\n",
    "\n",
    "How can we get the output formatting under control? We can add a String Formatter to the chain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "3rSDhR5AeR7_",
    "outputId": "f6a27217-9a3e-41c7-89d2-8c72d835dd5f"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "test_cohere_llm_chain_short_formatted = (\n",
    "    {\"object\": RunnablePassthrough()}\n",
    "    | test_llm_prompt_template\n",
    "    | cohere_chat_model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "test_cohere_llm_chain_short_formatted.invoke('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry1S5W_ueSWt"
   },
   "source": [
    "### 2.6 Setting Up a Simple RAG Chain\n",
    "\n",
    "For RAG, we will follow the same approach. Except... you will **later** need to change the chain to include the retrieval step.\n",
    "\n",
    "We first do a simple test: create a RAG template that takes a question and a pre-defined context as input, and generates the answer based on the provided context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrSx3gFlncAO",
    "outputId": "d295a81d-0c70-4dbe-dbe7-42d22248648a"
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"[INST] Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "base_rag_chain =(\n",
    "    {\"context\": RunnablePassthrough(),\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_template\n",
    "    | mistral_llm_lc\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "predefined_context = \"Germany has won the World Cup 4 times.\"\n",
    "question = \"How many times did Germany win the world cup?\"\n",
    "\n",
    "resp = base_rag_chain.invoke({'context': predefined_context,\n",
    "                           'question': question})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Maq5x1jDhJiX"
   },
   "source": [
    "That's great. But of course, the context needs to be created in an earlier retrieval step. More precisely, the documents will be first retrieved as a list, and then they will need to be formatted into one string to pass to the LLM in the context window.\n",
    "\n",
    "Here is a simple formatting function that can be hooked into the chain, which combines a list of chunks into one string:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VUMkGithJtY"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv0wLvGQhJ5R"
   },
   "source": [
    "So how could we build a simple chain? Let's first just get the retrieval done and the formatted retrieved data and the question inserted into the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UiGHgRLhKEZ"
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"Here is a context:\\n{context} \\n\\nand here is a question: \\n{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVRdhLzwjYk6"
   },
   "outputs": [],
   "source": [
    "output = rag_chain.invoke('What is Chain of Thought?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8UynrWikgPc"
   },
   "source": [
    "Ok... with some formatting... this looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjlKfsrljYnn",
    "outputId": "559c74dc-0c1a-4ac5-9b50-d5f0d5688fb0"
   },
   "outputs": [],
   "source": [
    "print(output.messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx0ZWB7Ul4x8"
   },
   "source": [
    "Let's complete the RAG Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh9ZopW3jYqA"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | mistral_llm_lc\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "_e_gx7lMjYse",
    "outputId": "df0f2d56-7b91-4475-c5f8-1e2a45c41a95"
   },
   "outputs": [],
   "source": [
    "rag_chain.invoke('What is Chain of Thought?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfNT72j2mZw-"
   },
   "source": [
    "What about the Cohere models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHye-j_MjYvL"
   },
   "outputs": [],
   "source": [
    "cohere_rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | cohere_chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "YnVEnMmNmrgM",
    "outputId": "6a1924c6-7fb4-437d-af2a-a5db1c8e20c6"
   },
   "outputs": [],
   "source": [
    "cohere_rag_chain.invoke('What is Chain of Thought?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtgVF-T8nN9o"
   },
   "source": [
    "Works too! Time to build the real thing and do experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM7gS9kGNOJp"
   },
   "source": [
    "## 3. The RAG Model & Experimentation\n",
    "\n",
    "With this we can get started. First, we need to acquire the data, chunk it, vectorize it, and store the embeddings (and in this simple case also the docs) in our Qdrant vector db.\n",
    "\n",
    "\n",
    "### 3.1 The Vector Database\n",
    "\n",
    "We will start by creating our datastore, Qdrant. Usually, you would deploy the vector db as a server, but in this case let's simply put everything in memory. Also, in this case we will store not only the embeddings but the whole document in the vector store. We will seed the store with the splits from the blog post we had used before.\n",
    "\n",
    "We will also create the retriever, which defines the way the documents are being retrieved. The retriever parameters define for example which method is used, how many docs are retrieved, etc. See [this LangChain link ](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXxjbq6RsKq3"
   },
   "outputs": [],
   "source": [
    "qdrant_vectorstore = Qdrant.from_documents(splits,\n",
    "    base_embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"rag_tech_db\",\n",
    "    force_recreate=True\n",
    ")\n",
    "\n",
    "retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC9z0o5ZsLML"
   },
   "source": [
    "### 3.2 Data Acquisition, Chunking, and Vectorization\n",
    "\n",
    "Now where we have our store we need to get the data into it. We will need to retrieve the data, create the chunks, then vectorize them, and finally store the vectors (along with the docs in this case) in the vector db.\n",
    "\n",
    "Let us first set chunk size and overlap, as well as the type of splitter. These are starting parameters and you may want to experiment with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_BR4rgYrCGn"
   },
   "outputs": [],
   "source": [
    "#Note that these defaults may or may not be ideal!\n",
    "CHUNK_SIZE=128\n",
    "OVERLAP=0\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-vhTZ_tvIvD"
   },
   "source": [
    "Now let's work with an actual document collection.  We will work with four types of documents:\n",
    "\n",
    "* A few papers from the ArXiv on RAG and NLP\n",
    "* A few blogs from Lily Weng that talk about Open Domain Question Answering and related topics\n",
    "* A number of Wikipedia articles on that topic\n",
    "\n",
    "To make testing easier  we'll define a global record number so we can trace back to see which chunk came from which specific document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEmdCyqqx5kl"
   },
   "outputs": [],
   "source": [
    "#assign a unique number to each document we ingest\n",
    "global_doc_number = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_xFjAUSx7ES"
   },
   "source": [
    "First we'll grab some papers from ArXiv.  We'll grab the pdf files and get all of the pages as separate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6Jj2vLAyxic"
   },
   "outputs": [],
   "source": [
    "arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
    "                 '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
    "                 '2311.08377', '2312.05708', '2401.06532', '2401.17268', '2402.01306', '2402.19473', '2406.04744')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq8ETxzzygTj"
   },
   "outputs": [],
   "source": [
    "all_arxiv_pages = []\n",
    "\n",
    "#loop through the papers\n",
    "for identifier in arxiv_numbers:\n",
    "    # Construct URL using the arXiv unique identifier\n",
    "    arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
    "\n",
    "    # Extract pages from the document and add them to the list of pages\n",
    "    arx_loader = PyMuPDFLoader(arx_url)\n",
    "    arx_pages = arx_loader.load()\n",
    "    for page_num in range(len(arx_pages)):\n",
    "        page = arx_pages[page_num]\n",
    "        #CHANGED\n",
    "        page.metadata['page_num'] = page_num\n",
    "        page.metadata['doc_num'] = global_doc_number\n",
    "        page.metadata['doc_source'] = \"ArXiv\"\n",
    "        all_arxiv_pages.append(page)\n",
    "\n",
    "\n",
    "    global_doc_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYMkGJ7igvg_"
   },
   "source": [
    "How many docs did we get?  Is that the correct number? And what is the content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXNS5MaMwrOK",
    "outputId": "1f718acf-85e7-448d-a79b-da90ea2fc81e"
   },
   "outputs": [],
   "source": [
    "num_pages = len(all_arxiv_pages)\n",
    "num_docs = global_doc_number - 1\n",
    "\n",
    "print(f\"{num_docs} documents in total\")\n",
    "print(f\"{num_pages} pages in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "f6Qr75rnvLKJ",
    "outputId": "c332fc46-21bf-4735-8a72-6805dd1ccd48"
   },
   "outputs": [],
   "source": [
    "all_arxiv_pages[5].page_content[:150]  # all pages of the Document content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Czx-h_Aeg_NM"
   },
   "source": [
    "Now we need to split the docs into chunks.  LangChain provides a couple of ways to do that.  We'll use for now the `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ln3nmeVvLBI",
    "outputId": "34828baf-afb3-45f8-ca4a-221925ca7ecd"
   },
   "outputs": [],
   "source": [
    "#index doc chunks\n",
    "splits = text_splitter.split_documents(all_arxiv_pages)\n",
    "for idx, text in enumerate(splits):\n",
    "    splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVEpjCBLE50m",
    "outputId": "b2d735d6-41b6-462b-a80b-d5fbf25ed703"
   },
   "outputs": [],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6DUxHWuozJB"
   },
   "source": [
    "Let's add the vectors to the datastore and see whether we can retrieve a nearest neighbor to a query. Let's look at the second closest match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSJmDkj6SvQQ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "qdrant_vectorstore.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xOhcgKd5ckk"
   },
   "outputs": [],
   "source": [
    "query = \"How can we train a model for preferences?\"\n",
    "found_docs = qdrant_vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDelZRCF5Ite",
    "outputId": "80886d5b-eefa-42de-cdfc-a4dc8490cf3b"
   },
   "outputs": [],
   "source": [
    "print(found_docs[0][0].page_content)\n",
    "print(found_docs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipT82FOghpr6"
   },
   "source": [
    "Next, let's get some information from Wikipedia on our main topic -- Gen AI.  LangChain provides a DocumentLoader that accesses the Wikipedia API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfWbJmg0vKfv",
    "outputId": "79b4df37-12aa-4d59-af4a-4508324a7b11"
   },
   "outputs": [],
   "source": [
    "wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n",
    "for idx, text in enumerate(wiki_docs):\n",
    "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#index docs\n",
    "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "for idx, text in enumerate(wiki_splits):\n",
    "    wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(wiki_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWD9eytoc7er"
   },
   "source": [
    "Now we'll add these splits to the vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC2LSdGwIcvn"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#vectorstore.add_documents(documents=wiki_splits, embedding=base_embeddings)\n",
    "qdrant_vectorstore.add_documents(documents=wiki_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsZSNcp4TWxO"
   },
   "source": [
    "Same with a couple of other queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlO0AYTmTaPM",
    "outputId": "da892913-4974-479b-c94b-420eb237b384"
   },
   "outputs": [],
   "source": [
    "wiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\n",
    "for idx, text in enumerate(wiki_docs):\n",
    "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#index docs\n",
    "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "for idx, text in enumerate(wiki_splits):\n",
    "    wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(wiki_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQqX-w8tTjah"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#vectorstore.add_documents(documents=wiki_splits, embedding=base_embeddings)\n",
    "qdrant_vectorstore.add_documents(documents=wiki_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtU-BeWpTkbv"
   },
   "source": [
    "And yet another related Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_uwyvZaTkx4",
    "outputId": "cc620b43-1a4a-4e66-99a1-99fb26a123f4"
   },
   "outputs": [],
   "source": [
    "wiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\n",
    "for idx, text in enumerate(wiki_docs):\n",
    "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#index docs\n",
    "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "for idx, text in enumerate(wiki_splits):\n",
    "    wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(wiki_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw-FQnNZTk7I"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#vectorstore.add_documents(documents=wiki_splits, embedding=base_embeddings)\n",
    "qdrant_vectorstore.add_documents(documents=wiki_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfs8ro_Ziekd"
   },
   "source": [
    "We'll also augment our collection with some blog entries about Open Domain Question Answering, of which RAG is an approach, and some related topics in case users want to ask how the new Search system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5LdggJw8nBF",
    "outputId": "ff8ff51f-3d58-4b37-f04d-a28c3980a0e9"
   },
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "               \"https://lilianweng.github.io/posts/2018-06-24-attention/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"),\n",
    "\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "web_documents = web_loader.load()\n",
    "\n",
    "for idx, text in enumerate(web_documents):\n",
    "    web_documents[idx].metadata['doc_num'] = global_doc_number\n",
    "    web_documents[idx].metadata['doc_source'] = \"WWW\"\n",
    "    global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(web_documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMjLDnHf8nBG"
   },
   "source": [
    "Again, we will split the retrieved data into chunks and add the data to the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D29aVbKn8nBH",
    "outputId": "5823306f-a6a0-4bd5-db17-141645f96a0b"
   },
   "outputs": [],
   "source": [
    "web_splits = text_splitter.split_documents(web_documents)\n",
    "\n",
    "for idx, text in enumerate(web_splits):\n",
    "    web_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits: ', len(web_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5slq33qt6Dc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "qdrant_vectorstore.add_documents(documents=web_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KVUbsDa3BBF"
   },
   "source": [
    "### 3.3 The Test Data\n",
    "\n",
    "You will want to test the system that you (will) have built. Below we give you a validation set that you could take as labeled data (imagine, your user personas would have had these questions and deemed the answers to be good). We also will give you a test set that only contains questions. (This is the set that we will use to get a feel for how well your RAG system corresponds to our Gold model).\n",
    "\n",
    "Here are is the gold validation set and the test questions. **DO NOT CHANGE OR DELETE!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxMnyWig7R-T"
   },
   "outputs": [],
   "source": [
    "validation_questions_answers = {\n",
    "    0: {\"question\": \"What purpose do large language models serve in the field of natural language processing?\",\n",
    "  \"gold_answer_research\": \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"},\n",
    "1: {\"question\": \"How does a large language model learn from text during training?\",\n",
    "  \"gold_answer_research\": \"A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\",\n",
    "  \"gold_answer_marketing\": \"A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\"},\n",
    "2: {\"question\": \"What are some key architectures behind the development of large language models?\",\n",
    "  \"gold_answer_research\": \"Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\",\n",
    "  \"gold_answer_marketing\": \"Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\"},\n",
    "3: {\"question\": \"Can you name some specific large language models and the companies or organizations that have developed them?\",\n",
    "  \"gold_answer_research\": \"Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla by DeepMind, GPT-3 by OpenAI.\"},\n",
    "7: {\"question\": \"What licensing models have been adopted for the distribution of source-available language models?\",\n",
    "  \"gold_answer_research\": \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\"},\n",
    "8: {\"question\": \"What are language models and what is their purpose in natural language processing?\",\n",
    "  \"gold_answer_research\": \"Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\",\n",
    "  \"gold_answer_marketing\": \"Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\"},\n",
    "9: {\"question\": \"How have language models evolved in terms of architecture, from the 1980s to present times?\",\n",
    "  \"gold_answer_research\": \"Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\",\n",
    "  \"gold_answer_marketing\": \"Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\"},\n",
    "11: {\"question\": \"Can you explain how maximum entropy language models work and what the partition function signifies?\",\n",
    "  \"gold_answer_research\": \"Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\",\n",
    "  \"gold_answer_marketing\": \"Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\"},\n",
    "12: {\"question\": \"What is the benefit of using continuous space embeddings in recurrent neural network language models?\",\n",
    "  \"gold_answer_research\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\",\n",
    "  \"gold_answer_marketing\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\"},\n",
    "13: {\"question\": \"What challenges do large language models face in mirroring human cognitive patterns?\",\n",
    "  \"gold_answer_research\": \"Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\",\n",
    "  \"gold_answer_marketing\": \"Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\"},\n",
    "16: {\"question\": \"What factors influenced the development of generative language models by Anthropic?\",\n",
    "  \"gold_answer_research\": \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\",\n",
    "  \"gold_answer_marketing\": \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"},\n",
    "17: {\"question\": \"What is Constitutional AI and how does it affect the functionality of AI systems?\",\n",
    "  \"gold_answer_research\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\",\n",
    "  \"gold_answer_marketing\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\"},\n",
    "18: {\"question\": \"How do advances in AI models impact their ability to interact with different types of data, such as images?\",\n",
    "  \"gold_answer_research\": \"Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\",\n",
    "  \"gold_answer_marketing\": \"Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\"},\n",
    "19: {\"question\": \"What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\",\n",
    "  \"gold_answer_research\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\"},\n",
    "20: {\"question\": \"How has the token handling capacity changed between different versions of the Claude model?\",\n",
    "  \"gold_answer_research\": \"The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\",\n",
    "  \"gold_answer_marketing\": \"The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\"},\n",
    "22: {\"question\": \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\",\n",
    "  \"gold_answer_research\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\",\n",
    "  \"gold_answer_marketing\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\"},\n",
    "23: {\"question\": \"How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\",\n",
    "  \"gold_answer_research\": \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\"},\n",
    "24: {\"question\": \"Who developed the language model family known as Chinchilla?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\",\n",
    "  \"gold_answer_marketing\": \"The research team at DeepMind developed the language model family known as Chinchilla.\"},\n",
    "25: {\"question\": \"What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\",\n",
    "  \"gold_answer_research\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\"},\n",
    "27: {\"question\": \"What is the relationship between Chinchilla and the Gopher language model families?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\"},\n",
    "28: {\"question\": \"What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\",\n",
    "  \"gold_answer_research\": \"The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\"},\n",
    "30: {\"question\": \"What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\"},\n",
    "33: {\"question\": \"What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\",\n",
    "  \"gold_answer_research\": \"Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\",\n",
    "  \"gold_answer_marketing\": \"Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\"},\n",
    "34: {\"question\": \"What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\",\n",
    "  \"gold_answer_research\": \"One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\",\n",
    "  \"gold_answer_marketing\": \"Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\"},\n",
    "35: {\"question\": \"How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\",\n",
    "  \"gold_answer_research\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\",\n",
    "  \"gold_answer_marketing\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\"},\n",
    "36: {\"question\": \"What is the significance of comparing the normalized subspace similarity between âˆ†Wq, âˆ†Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\",\n",
    "  \"gold_answer_research\": \"Comparing the normalized subspace similarity between âˆ†Wq, âˆ†Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix âˆ†W and sheds light on the connection between âˆ†W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Comparing the normalized subspace similarity between âˆ†Wq, âˆ†Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\"},\n",
    "38: {\"question\": \"What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\",\n",
    "  \"gold_answer_research\": \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\",\n",
    "  \"gold_answer_marketing\": \"The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\"},\n",
    "39: {\"question\": \"What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\",\n",
    "  \"gold_answer_research\": \"Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\",\n",
    "  \"gold_answer_marketing\": \"Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\"},\n",
    "41: {\"question\": \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\",\n",
    "  \"gold_answer_research\": \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\"},\n",
    "43: {\"question\": \"What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\",\n",
    "  \"gold_answer_research\": \"The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\",\n",
    "  \"gold_answer_marketing\": \"The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\"},\n",
    "44: {\"question\": \"What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\",\n",
    "  \"gold_answer_research\": \"Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\",\n",
    "  \"gold_answer_marketing\": \"The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\"},\n",
    "45: {\"question\": \"What are some challenges in training retrieval systems and how are negative samples used to address them?\",\n",
    "  \"gold_answer_research\": \"Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\",\n",
    "  \"gold_answer_marketing\": \"Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\"},\n",
    "46: {\"question\": \"What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\",\n",
    "  \"gold_answer_research\": \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\",\n",
    "  \"gold_answer_marketing\": \"Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\"},\n",
    "47: {\"question\": \"What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\",\n",
    "  \"gold_answer_research\": \"Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\",\n",
    "  \"gold_answer_marketing\": \"Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\"},\n",
    "48: {\"question\": \"What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\",\n",
    "  \"gold_answer_research\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\",\n",
    "  \"gold_answer_marketing\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\"},\n",
    "50: {\"question\": \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\",\n",
    "  \"gold_answer_research\": \"To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\",\n",
    "  \"gold_answer_marketing\": \"Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\"},\n",
    "51: {\"question\": \"Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\",\n",
    "  \"gold_answer_research\": \"Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\",\n",
    "  \"gold_answer_marketing\": \"The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\"},\n",
    "52: {\"question\": \"What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\",\n",
    "  \"gold_answer_research\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\",\n",
    "  \"gold_answer_marketing\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\"},\n",
    "54: {\"question\": \"What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\",\n",
    "  \"gold_answer_research\": \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\",\n",
    "  \"gold_answer_marketing\": \"Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\"},\n",
    "55: {\"question\": \"What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\",\n",
    "  \"gold_answer_research\": \"The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\",\n",
    "  \"gold_answer_marketing\": \"The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\"},\n",
    "59: {\"question\": \"Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\",\n",
    "  \"gold_answer_research\": \"To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\"},\n",
    "60: {\"question\": \"What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\",\n",
    "  \"gold_answer_research\": \"Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\",\n",
    "  \"gold_answer_marketing\": \"Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\"},\n",
    "61: {\"question\": \"What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\",\n",
    "  \"gold_answer_research\": \"One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\",\n",
    "  \"gold_answer_marketing\": \"Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\"},\n",
    "62: {\"question\": \"What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\",\n",
    "  \"gold_answer_research\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\",\n",
    "  \"gold_answer_marketing\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\"},\n",
    "63: {\"question\": \"What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\",\n",
    "  \"gold_answer_research\": \"The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\",\n",
    "  \"gold_answer_marketing\": \"The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\"},\n",
    "64: {\"question\": \"What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\",\n",
    "  \"gold_answer_research\": \"In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\",\n",
    "  \"gold_answer_marketing\": \"The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\"},\n",
    "65: {\"question\": \"What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\",\n",
    "  \"gold_answer_research\": \"Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\",\n",
    "  \"gold_answer_marketing\": \"Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\"},\n",
    "67: {\"question\": \"What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\",\n",
    "  \"gold_answer_research\": \"Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\",\n",
    "  \"gold_answer_marketing\": \"The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\"},\n",
    "69: {\"question\": \"What is the role of manual assessment in the validation of language model predictions according to the text provided?\",\n",
    "  \"gold_answer_research\": \"Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\"},\n",
    "70: {\"question\": \"What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\",\n",
    "  \"gold_answer_research\": \"The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\",\n",
    "  \"gold_answer_marketing\": \"The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\"},\n",
    "73: {\"question\": \"What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\",\n",
    "  \"gold_answer_research\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\",\n",
    "  \"gold_answer_marketing\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\"},\n",
    "74: {\"question\": \"What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\",\n",
    "  \"gold_answer_research\": \"Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\",\n",
    "  \"gold_answer_marketing\": \"Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\"},\n",
    "75: {\"question\": \"Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\",\n",
    "  \"gold_answer_research\": \"Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\",\n",
    "  \"gold_answer_marketing\": \"Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\"},\n",
    "76: {\"question\": \"What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\",\n",
    "  \"gold_answer_research\": \"A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\",\n",
    "  \"gold_answer_marketing\": \"A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\"},\n",
    "78: {\"question\": \"What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\",\n",
    "  \"gold_answer_research\": \"According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\",\n",
    "  \"gold_answer_marketing\": \"The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\"},\n",
    "80: {\"question\": \"How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\",\n",
    "  \"gold_answer_research\": \"Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\",\n",
    "  \"gold_answer_marketing\": \"KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\"},\n",
    "81: {\"question\": \"What are some common approaches to building an open-domain question answering system?\",\n",
    "  \"gold_answer_research\": \"Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\",\n",
    "  \"gold_answer_marketing\": \"Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\"},\n",
    "82: {\"question\": \"What is the difference between open-book and closed-book question answering?\",\n",
    "  \"gold_answer_research\": \"Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\",\n",
    "  \"gold_answer_marketing\": \"Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\"},\n",
    "84: {\"question\": \"What are the basic components of the Retriever-Reader framework in open-domain QA?\",\n",
    "  \"gold_answer_research\": \"The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\",\n",
    "  \"gold_answer_marketing\": \"The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\"},\n",
    "85: {\"question\": \"How is the TF-IDF model used in question answering retrieval systems?\",\n",
    "  \"gold_answer_research\": \"In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\"},\n",
    "86: {\"question\": \"Can neural networks enhance the process of information retrieval in QA systems?\",\n",
    "  \"gold_answer_research\": \"Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\",\n",
    "  \"gold_answer_marketing\": \"Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\"},\n",
    "87: {\"question\": \"What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\",\n",
    "  \"gold_answer_marketing\": \"Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\"},\n",
    "88: {\"question\": \"How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\",\n",
    "  \"gold_answer_marketing\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\"},\n",
    "89: {\"question\": \"What is the main goal of prompt engineering in language models?\",\n",
    "  \"gold_answer_research\": \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n",
    "  \"gold_answer_marketing\": \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\"},\n",
    "91: {\"question\": \"What are some known biases that can affect the performance of few-shot classification in LLMs?\",\n",
    "  \"gold_answer_research\": \"Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\",\n",
    "  \"gold_answer_marketing\": \"Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\"},\n",
    "92: {\"question\": \"Why might increasing model size not reduce variance in model performance with varying prompts?\",\n",
    "  \"gold_answer_research\": \"Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\",\n",
    "  \"gold_answer_marketing\": \"Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\"},\n",
    "93: {\"question\": \"What is the benefit of instruction-based finetuning in language models?\",\n",
    "  \"gold_answer_research\": \"Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\",\n",
    "  \"gold_answer_marketing\": \"The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\"},\n",
    "94: {\"question\": \"Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\",\n",
    "  \"gold_answer_research\": \"Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\",\n",
    "  \"gold_answer_marketing\": \"Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\"},\n",
    "95: {\"question\": \"What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\",\n",
    "  \"gold_answer_research\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\",\n",
    "  \"gold_answer_marketing\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\"},\n",
    "96: {\"question\": \"How do augmented language models with external tools differ from regular models in functionality?\",\n",
    "  \"gold_answer_research\": \"Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\",\n",
    "  \"gold_answer_marketing\": \"Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\"},\n",
    "97: {\"question\": \"What can be inferred about the utilization of attention in neural networks?\",\n",
    "  \"gold_answer_research\": \"Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\",\n",
    "  \"gold_answer_marketing\": \"Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\"},\n",
    "101: {\"question\": \"Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\",\n",
    "  \"gold_answer_research\": \"Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\",\n",
    "  \"gold_answer_marketing\": \"Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\"},\n",
    "102: {\"question\": \"What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\",\n",
    "  \"gold_answer_research\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\",\n",
    "  \"gold_answer_marketing\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\"},\n",
    "103: {\"question\": \"How does the transformer model variate from traditional sequence-aligned recurrent architectures?\",\n",
    "  \"gold_answer_research\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\"},\n",
    "104: {\"question\": \"What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\",\n",
    "  \"gold_answer_research\": \"The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\",\n",
    "  \"gold_answer_marketing\": \"The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\"},\n",
    "}\n",
    "\n",
    "\n",
    "test_questions = {\n",
    "4: {\"question\": \"When was the transformer architecture introduced, and by which organization?\"},\n",
    "5: {\"question\": \"How has the accessibility of powerful language models, such as GPT-3 and GPT-4, been controlled by their developers?\"},\n",
    "6: {\"question\": \"What benchmarks or ratings are used to compare the capabilities of different language models?\"},\n",
    "10: {\"question\": \"What are some of the primary applications for language models in technology and computing?\"},\n",
    "14: {\"question\": \"How are language models typically evaluated and what benchmarks are used for this purpose?\"},\n",
    "15: {\"question\": \"What datasets are available for evaluating language processing systems?\"},\n",
    "21: {\"question\": \"What collaborations with other companies have contributed to the development of Claude's capabilities?\"},\n",
    "26: {\"question\": \"According to DeepMind, how should the number of training tokens change relative to the model size?\"},\n",
    "29: {\"question\": \"How do the sizes of models in the Gopher family range?\"},\n",
    "31: {\"question\": \"What type of model architecture do the Gopher and Chinchilla families belong to?\"},\n",
    "32: {\"question\": \"Can you name the author who wrote the novels A Farewell to Arms and The Sun Also Rises?\"},\n",
    "37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n",
    "40: {\"question\": \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"},\n",
    "42: {\"question\": \"What types of evaluation metrics are commonly used to assess the accuracy of answers in AI-driven question and answer datasets?\"},\n",
    "49: {\"question\": \"What factors contribute to the performance improvement in retrieval-augmented language models compared to non-retrieval-augmented models?\"},\n",
    "56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n",
    "57: {\"question\": \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\"},\n",
    "58: {\"question\": \"What methods have been discussed in the literature for improving the alignment of language models with human preferences or feedback?\"},\n",
    "66: {\"question\": \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"},\n",
    "68: {\"question\": \"Consider a document related to research in natural language processing or artificial intelligence. Can you name some of the recent topics or methods that have been discussed or introduced in the field according to the document?\"},\n",
    "71: {\"question\": \"What is the significance of using reflection tokens in a model like SELF-RAG?\"},\n",
    "72: {\"question\": \"How does the inclusion of selected context as opposed to appending all retrieved text spans impact computational cost during both training and inference times in language model generation tasks?\"},\n",
    "77: {\"question\": \"What are the benefits of modeling human biases in Human-Aware Loss Optimizations (HALOs), and how do they compare to non-HALOs on the same datasets?\"},\n",
    "79: {\"question\": \"What are the modifications made to the traditional Kahneman-Tversky model to adapt it for optimizing language model performance?\"},\n",
    "83: {\"question\": \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"},\n",
    "90: {\"question\": \"How can adding examples to a prompt affect the performance of language models?\"},\n",
    "98: {\"question\": \"What are the main components of a Neural Turing Machine (NTM) architecture?\"},\n",
    "99: {\"question\": \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\"},\n",
    "100: {\"question\": \"What differentiates hard attention from soft attention in image processing algorithms?\"},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC86ggPEuipg"
   },
   "source": [
    "### 3.3 Running the RAG System\n",
    "\n",
    "Let's have a quick look at the validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABdr23NUSbTj",
    "outputId": "bf825bfd-75d0-4e99-fbdf-fedb74d67e06"
   },
   "outputs": [],
   "source": [
    "validation_questions_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ra_p5DhzSbeN",
    "outputId": "7e412658-a758-4b85-c0f9-bd98d68f84be"
   },
   "outputs": [],
   "source": [
    "test_questions[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FQbNGECSq1g"
   },
   "source": [
    "Let's now use the data to ask questions against it. So we need to define our prompt templates, the RAG Chain, etc.\n",
    "\n",
    "We have two types of User Personas we need to support:\n",
    "\n",
    "1. The engineers, who require pretty detailed information when they ask questions  \n",
    "2. The marketing team and supporting staff who also will ask questions around GenAI in order to better understand the products and the field as a whole, but a lot more high level answers would likely be in order\n",
    "\n",
    "**Below, please build your RAG pipeline including the relevant prompts. This is free form so you will need to create your own cells, text documentation as you need, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJE1wexvV7ay"
   },
   "source": [
    "##4. Tests & Evaluations\n",
    "\n",
    "Here you should evaluate the results. First, you should define your evaluation metrics and then you should run evaluation tests. This is really your area, but key results to show are:\n",
    "\n",
    "1) Your metrics of choice  \n",
    "2) How  your various models compare to the labeled validation data.\n",
    "\n",
    "Make sure you look at the results for the marketing team and the research team separately.\n",
    "\n",
    "**Note:** You do not need to run all models against all labeled questions, as that may take some time. Just do that for a few models/configs, and test a larger set with a smaller subset.\n",
    "\n",
    "**This is free form so you will need to create your own cells, text documentation as you need, etc.**\n",
    "\n",
    "### 4.1. Metrics\n",
    "\n",
    "Please define and motivate your metrics here. Please feel free to add more text and code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjOCg-BHWBvY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKktTilyTy_z"
   },
   "source": [
    "### 4.2. Evaluation Comparisons\n",
    "\n",
    "Document your key runs here. Feel free to add more text and code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXSFKKvtT0m1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdOV07NeT0bC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdOhlN23AYiA"
   },
   "source": [
    "## 5. Results\n",
    "\n",
    "### 5.1 Model Specifications\n",
    "\n",
    "Document the detailed specs of your choices. Also comment on how you valued the needs of the marketing tean vs the needs of the researchers, in case you had to make a trade-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSoSdUriAa-C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7J41pWiyh06"
   },
   "source": [
    "\n",
    "### 5.2 Some Test Questions\n",
    "\n",
    "**QUESTIONS:**\n",
    "\n",
    "\n",
    "Please study the answers generated by your chosen setup for these specific test questions:\n",
    "\n",
    "1. \"What purpose do large language models serve in the field of natural language processing?\" (Question 0)\n",
    "\n",
    "2. \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\" (Question 50)\n",
    "\n",
    "3. \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\" (Question 83, no labeled answers)\n",
    "\n",
    "For each of the three questions above please provide:\n",
    "\n",
    "a) The RAG results (research and marketing response)  \n",
    "b) The context provided  \n",
    "c) The document sources for the context  \n",
    "d) Also discuss your metric(s) for the first two examples (for both responses) compared to the gold responses\n",
    "\n",
    "Then, for questions 1 and 2, comment on how well you feel your metrics captured the differences and similarities between your answer and the gold answer?\n",
    "\n",
    "Put your answers to these questions into the answers file as you have done on previous assignments. Please consult the answer file for further details.\n",
    "\n",
    "#### 5.2.1 Test Question 1\n",
    "\n",
    "Please run the query:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgJbWDR7Ab-K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6wrr5VUUJk1"
   },
   "source": [
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZdkNySoUDy3"
   },
   "source": [
    "#### 5.2.2 Test Question 2\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izSpKhLHT5Ky"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDovfhEtUMMr"
   },
   "source": [
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxDhzMXsT48H"
   },
   "source": [
    "#### 5.2.3 Test Question 3\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WXJX_x3T4we"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24UZvRmhUOWk"
   },
   "source": [
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44h038A7AUdn"
   },
   "source": [
    "### 5.3 Other Questions\n",
    "\n",
    "Below are a few questions that you should think about. Please answer them in the answer file directly (in a short paragraph) and also see whether they may be relevant for your final write-up.\n",
    "\n",
    "**QUESTION:**\n",
    "\n",
    "5.3.a. How would you expect your response quality to change if you had a chunk size of 50?\n",
    "\n",
    "5.3.b. How would you expect your response quality to change if you had a chunk size of 5000?\n",
    "\n",
    "5.3.c. If you had time, how do you think fine-tuning of the LLM could help?  What type of data would you want for that? And which training approach would you take?\n",
    "\n",
    "5.3.d. What was your design philosophy  of the prompts? How did they differ between engineering and marketing support?\n",
    "\n",
    "5.3.e. What are your average and peak load estimates for the system? Given that, would you suggest a pay-per-use deployment or one that reserves the LLM?\n",
    "\n",
    "5.3.f. What type of limitations/risks would you see in using this system?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yhZpA9y1HyN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
