# Write your short answers in this file, replacing the
# placeholders (0, 0.0000, [0,.., ], 'abcde', abcde, your answer here,...) as appropriate.
# For numbers, the placeholder indicates the significant digits we expect.
#
# This assignment consists of multiple parts within one notebook.
#
# See the Description for details
#


###################################################################
###################################################################
## Retrieval Augmented Generation 
###################################################################
###################################################################

# ------------------------------------------------------------------
# | Section (0): All Test Questions    |
# ------------------------------------------------------------------

# Question 0.a.i: What is the prompt you used for the engineering research group ?
test_question_0_a_i: You work in the research team at a startup company delivering Generative AI based products.
Review the provided context, then tailor your answer to the {persona} team.
You should ignore pieces of the context that are irrelevant formatting, such as html or unrelated academic citations.
Use only the provided context to answer the question.
Identify the key information in the context that directly answers the question.
If the context alone does not provide enough information to answer the specific question, clearly state that the topic isn't explicitly available in the context, then use the contextual information to provide the closest possible answer.
Ensure your answer is provided as human-readable sentences.  It should not include excessive formatting or lists.
As an engineering-focused team member, prioritize technical accuracy and details when formulating your answer.\nUse precise terminology, explain concepts thoroughly, and avoid oversimplification.\nYour answer should not exceed 125 words.
Your answer should address the provided question without including extraneous or inpertinent information.

### Context:
{context}

### Question:
{question}


# Question 0.a.ii: What is the prompt you used for the marketing group??
test_question_0_a_ii: You work in the marketing team at a startup company delivering Generative AI based products.
Review the provided context, then tailor your answer to the {persona} team.
You should ignore pieces of the context that are irrelevant formatting, such as html or unrelated academic citations.
Use only the provided context to answer the question.
Identify the key information in the context that directly answers the question.
If the context alone does not provide enough information to answer the specific question, clearly state that the topic isn't explicitly available in the context, then use the contextual information to provide the closest possible answer.
Ensure your answer is provided as human-readable sentences.  It should not include excessive formatting or lists.
As a marketing-focused team member, prioritize clarity and brevity when formulating your answer.\nUse simple, relatable language and focus on the key takeaways without getting bogged down in too much technical jargon.\nYour answer should not exceed 75 words.
Your answer should address the provided question without including extraneous or inpertinent information.

### Context:
{context}

### Question:
{question}




# ------------------------------------------------------------------
# | Section (1): Test Question 1 (Question 0 in validation set)    |
# ------------------------------------------------------------------


# Question 1.a.i: What is the engineering response for question 1?
test_question_1_a_i: A large language model acquires its ability to process and generate natural language by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process. It uses techniques like autoregressive language modeling and denoising autoencoders to understand context and generate responses. The largest and most capable models are typically neural networks with decoder-only transformer-based architectures, allowing for efficient handling of large-scale text data. These models can acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.


# Question 1.a.ii: What is the marketing response for question 1?
test_question_1_a_ii: A large language model acquires knowledge from text during training through a self-supervised and semi-supervised process. It learns statistical relationships from vast amounts of text data. The largest and most capable models are built with decoder-only transformer-based architectures, allowing for efficient processing and generation of large-scale text data. These models can be fine-tuned for specific tasks or guided by prompt engineering. Despite their ability to acquire syntactic, semantic, and ontological understanding, they also inherit biases and inaccuracies present in the data they are trained on.


# Question 1.b: What is the context you passed to the LLM for question 1?
test_question_1_b: \nA large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n\n\n== History ==\n\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets ("web as corpus"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\n\nA language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\n\n\n== Pure statistical models ==\n\n\n=== Models based on word n-grams ===\n\n\n=== Exponential ===\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\n\nthat meaningfully represents the people impacted by the technology, and that synthesizes peoples’\nvalues in a way that achieves broad consensus amongst many groups. We discuss some related\nconsiderations in Section 5.2.\n5.5\nBroader impacts\nThis work is motivated by our aim to increase the positive impact of large language models by training\nthem to do what a given set of humans want them to do. By default, language models optimize\nthe next word prediction objective, which is only a proxy for what we want these models to do.\nOur results indicate that our techniques hold promise for making language models more helpful,\ntruthful, and harmless. In the longer term, alignment failures could lead to more severe consequences,\nparticularly if these models are deployed in safety-critical situations. We expect that as model scaling\ncontinues, greater care has to be taken to ensure that they are aligned with human intentions (Bostrom,\n2014).\nHowever, making language models better at following user intentions also makes them easier to\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\nabusive content.\nAlignment techniques are not a panacea for resolving safety issues associated with large language\nmodels; rather, they should be used as one tool in a broader safety ecosystem. Aside from intentional\nmisuse, there are many domains where large language models should be deployed only with great\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\npeople based on protected characteristics, determining eligibility for credit, employment, or hous-\ning, generating political advertisements, and law enforcement. If these models are open-sourced,\nit becomes challenging to limit harmful applications in these and other domains without proper\nregulation. On the other hand, if large language model access is restricted to a few organizations\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\n\npeople based on protected characteristics, determining eligibility for credit, employment, or hous-\ning, generating political advertisements, and law enforcement. If these models are open-sourced,\nit becomes challenging to limit harmful applications in these and other domains without proper\nregulation. On the other hand, if large language model access is restricted to a few organizations\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\ntechnology. Another option is for an organization to own the end-to-end infrastructure of model\ndeployment, and make it accessible via an API. This allows for the implementation of safety protocols\nlike use case restriction (only allowing the model to be used for certain applications), monitoring\nfor misuse and revoking access to those who misuse the system, and rate limiting to prevent the\ngeneration of large-scale misinformation. However, this can come at the cost of reduced transparency\nand increased centralization of power because it requires the API provider to make decisions on\nwhere to draw the line on each of these questions.\nFinally, as discussed in Section 5.2, the question of who these models are aligned to is extremely\nimportant, and will signiﬁcantly affect whether the net impact of these models is positive or negative.\n20\n\nsystems. arXiv preprint arXiv:1905.00537.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWeidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M.,\nBalle, B., Kasirzadeh, A., et al. (2021). Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359.\nWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli,\nP., Coppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. arXiv\npreprint arXiv:2109.07445.\nWu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. (2021).\nRecursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.\nXu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein, D. (2021). Detoxifying language\nmodels risks marginalizing minority voices. arXiv preprint arXiv:2104.06390.\nXu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. (2020). Recipes for safety in\nopen-domain chatbots. arXiv preprint arXiv:2010.07079.\nYi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh, A., Gabriel, R., and\nHakkani-Tur, D. (2019). Towards coherent and engaging spoken dialog response generation\nusing automatic conversation evaluators. arXiv preprint arXiv:1904.13015.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). Hellaswag: Can a machine\nreally ﬁnish your sentence? In Association for Computational Linguistics, pages 4791–4800.\nZhao, M., Anderson, P., Jain, V., Wang, S., Ku, A., Baldridge, J., and Ie, E. (2021). On the evaluation\nof vision-and-language navigation instructions. arXiv preprint arXiv:2101.10504.\nZhou, W. and Xu, K. (2020). Learning to compare for better training and evaluation of open domain\nnatural language generation models. arXiv preprint arXiv:2002.05058.\n\nLlama (Large Language Model Meta AI, formerly stylized as LLaMA) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3.3, released in December 2024.\nModel weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 1B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n\n\n== Background ==\nAfter the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\nCompared with other responses to ChatGPT, Meta\'s Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\nAn empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the "Chinchilla-optimal" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022.\nFinetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369–2380. Association for Computational\nLinguistics.\nHamed Zamani, Gord Lueck, Everest Chen, Rodolfo\nQuispe, Flint Luu, and Nick Craswell. 2020. MIM-\nICS: A large-scale data collection for search clarifi-\ncation. In CIKM ’20: The 29th ACM International\nConference on Information and Knowledge Manage-\nment, Virtual Event, Ireland, October 19-23, 2020,\npages 3189–3196. ACM.\nChen Zhang, Dawei Song, Zheyu Ye, and Yan Gao.\n2023a. Towards the law of capacity gap in distilling\nlanguage models. CoRR, abs/2311.07052.\nLonghui Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, Meishan Zhang, and Min Zhang. 2023b.\nRankinggpt: Empowering large language models in\ntext ranking with progressive enhancement. CoRR,\nabs/2311.16720.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Zhicheng Dou, and\nJi-Rong Wen. 2023. Large language models for infor-\nmation retrieval: A survey. CoRR, abs/2308.07107.\nShengyao Zhuang, Bing Liu, Bevan Koopman, and\nGuido Zuccon. 2023. Open-source large language\nmodels are strong zero-shot query likelihood models\nfor document ranking. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2023,\nSingapore, December 6-10, 2023, pages 8807–8817.\nAssociation for Computational Linguistics.\nA\nDetails about Tasks & Datasets\nWe introduce the tasks and datasets as follows.\n\n1. Introduction\nLarge language models (LLMs) (Anthropic, 2023; Brown et al., 2020; Google, 2023; Jiang et al.,\n2023a; OpenAI, 2022, 2023; Radford et al., 2018, 2019; Gemini Team, 2023; Touvron et al., 2023a,b;\nYin et al., 2023; Zhao et al., 2023) based on Transformers (Vaswani et al., 2017) have become a\nprominent pathway to Artificial General Intelligence (AGI). LLMs acquire massive world knowledge\nby learning to predict the next word on large-scale web corpora. The capabilities of LLMs have been\ncontinuously increasing by scaling model sizes, dataset sizes, and computation. After pre-training,\nLLMs can be aligned to support real-world use cases by supervised fine-tuning (Chung et al., 2022;\nSanh et al., 2022) and preference optimization techniques including reinforcement learning from\nhuman feedback (RLHF) (Ouyang et al., 2022a; Wang et al., 2024; Zheng et al., 2023b) and direct\npreference optimization (DPO) (Rafailov et al., 2023). The capabilities of LLMs have empowered\nvarious applications including ChatGPT, Claude, Bard, Microsoft Copilot, Character.AI, Notion AI, etc.\nRecently, many specialized LLMs have been trained for different targeted usage scenarios. In general,\nLLMs specialize according to the targeted domains (e.g., finance (Wu et al., 2023), healthcare (Yang\net al., 2022b), legal (Cui et al., 2023), etc.) and tasks (e.g., role-playing (Wang et al., 2023d),\ncoding (Rozière et al., 2023), etc.). However, the ability of LLMs to write human-like texts and\nproduce creative content, which is a critical use case of LLM applications such as ChatGPT, is mostly\noverlooked by the community.\nIn this report, we focus on the literature domain and the task of writing, or content creation,\nand introduce Weaver, a family of LLMs dedicatedly pre-trained and aligned for this purpose. The\nname "Weaver" symbolizes the model’s proficiency in skillfully amalgamating linguistic elements,\nakin to the way a craftsman weaves threads to form a fabric. We answer four main questions in this\n\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024, The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\nAs of 2024, the largest and most capable models are all based on the Transformer architecture. Some recent implementations are based o\n\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger,\nG., Kim, J. W., Kreps, S., et al. (2019). Release strategies and the social impacts of language\nmodels. arXiv preprint arXiv:1908.09203.\nSolaiman, I. and Dennison, C. (2021). Process for adapting language models to society (palms) with\nvalues-targeted datasets. arXiv preprint arXiv:2106.10328.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D.,\nand Christiano, P. (2020). Learning to summarize from human feedback. arXiv preprint\narXiv:2009.01325.\nTamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021). Understanding the capabilities,\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\nT., Baker, L., Du, Y., et al. (2022). Lamda: Language models for dialog applications. arXiv\npreprint arXiv:2201.08239.\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. M. (2020).\nInvestigating gender bias in language models using causal mediation analysis. In NeurIPS.\nVölske, M., Potthast, M., Syed, S., and Stein, B. (2017). Tl; dr: Mining reddit to learn automatic\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages\n59–63.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman,\nS. R. (2019). Superglue: A stickier benchmark for general-purpose language understanding\nsystems. arXiv preprint arXiv:1905.00537.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWeidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M.,\nBalle, B., Kasirzadeh, A., et al. (2021). Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359.\n\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\n2002.08910.\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/1500000019.\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\nArXiv, abs/1908.09203, 2019.\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n14\n\nGuy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent\nSifre. An empirical analysis of compute-optimal large language model training. In Advances in\nNeural Information Processing Systems, volume 35, 2022.\n[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\nTriviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension.\narXiv preprint\narXiv:1705.03551, 2017.\n[16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, 7:453–466, 2019.\n8\n\n[29] K. Sun, Y. E. Xu, H. Zha, Y. Liu, and X. L. Dong. Head-to-Tail: How knowledgeable are\nlarge language models (llms)? a.k.a. will llms replace knowledge graphs?\narXiv preprint\narXiv:2308.10168, 2024.\n[30] Y. Sun, H. Xin, K. Sun, Y. E. Xu, X. Yang, X. L. Dong, N. Tang, and L. Chen. Are large\nlanguage models a good replacement of taxonomies? Proc. VLDB Endow., 17(11):2919–2932,\naug 2024.\n[31] A. Talmor and J. Berant. The web as a knowledge-base for answering complex questions, 2018.\n[32] N. Tang, C. Yang, J. Fan, L. Cao, Y. Luo, and A. Y. Halevy. Verifai: Verified generative AI. In\nCIDR, 2024.\n[33] Y. Tang and Y. Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-\nhop queries. arXiv preprint arXiv:2401.15391, 2024.\n[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[35] R. Usbeck, X. Yan, A. Perevalov, L. Jiang, J. Schulz, A. Kraft, C. Möller, J. Huang, J. Reineke,\nA.-C. Ngonga Ngomo, et al. QALD-10–the 10th challenge on question answering over linked\ndata. Semantic Web, (Preprint):1–15, 2023.\n[36] T. Vu, M. Iyyer, X. Wang, N. Constant, J. Wei, J. Wei, C. Tar, Y.-H. Sung, D. Zhou, Q. Le, and\nT. Luong. FreshLLMs: Refreshing large language models with search engine augmentation,\n2023.\n\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n\naugmented language model pre-training.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\nParanjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. 2023. Lost in the middle: How lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nOpenAI. 2023. Gpt-4 technical report.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, and Michael Bender-\nsky. 2023. Large language models are effective text\nrankers with pairwise ranking prompting.\narXiv\npreprint arXiv:2306.17563v1.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\n\n


# Question 1.c: List the doc_source for each of the documents in the context for question 1
test_question_1_c: [{'score': 0.8523042940858748,
  'source': 'https://en.wikipedia.org/wiki/Large_language_model'},
 {'score': 0.840713054040978,
  'source': 'https://en.wikipedia.org/wiki/Language_model'},
 {'score': 0.8399481783250538,
  'source': 'https://arxiv.org/pdf/2305.14314.pdf'},
 {'score': 0.8370457870165853,
  'source': 'https://arxiv.org/pdf/2310.11511.pdf'},
 {'score': 0.8330776826097663,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'},
 {'score': 0.8317056199305001,
  'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/'},
 {'score': 0.8270514126850912,
  'source': 'https://arxiv.org/pdf/2005.11401.pdf'},
 {'score': 0.8268811554908028,
  'source': 'https://arxiv.org/pdf/2310.06825.pdf'},
 {'score': 0.8262030267682217,
  'source': 'https://arxiv.org/pdf/2305.14314.pdf'},
 {'score': 0.8211785076972027,
  'source': 'https://arxiv.org/pdf/2401.17268.pdf'},
 {'score': 0.8206325052874911,
  'source': 'https://arxiv.org/pdf/2306.15595.pdf'},
 {'score': 0.8202385771480054,
  'source': 'https://arxiv.org/pdf/2305.14314.pdf'},
 {'score': 0.819410323595906, 'source': 'https://arxiv.org/pdf/2104.07567.pdf'},
 {'score': 0.8182449119932688,
  'source': 'https://arxiv.org/pdf/2310.11511.pdf'},
 {'score': 0.8177398091928612,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'}]


# Question 1.d.i: How well does your model perform relative to the gold answer we provided for engineering on question 1?
test_question_1_d_i: ['answer_relevance_score': 4),
                  ('avg_chunk_score': 0.8288249897245074),
                  ('comprehensive_score': 2.5),
                  ('context_relevance_score': 2),
                  ('failthfulness_score': 1)]


# Question 1.d.ii:  How well does your model perform relative to the gold answer we provided for marketing on question 1?
test_question_1_d_ii: [('answer_relevance_score': 5),
                    ('avg_chunk_score': 0.8288249897245074),
                   ('comprehensive_score': 3.5),
                   ('context_relevance_score': 5),
                   ('failthfulness_score': 3)]


# ------------------------------------------------------------------
# | Section (2): Test Question 2 (Question 50 in validation set)    |
# ------------------------------------------------------------------


# Question 2.a.i: What is the engineering response for question 2?
test_question_2_a_i: Some key architectures behind the development of large language models include Transformer models (Vaswani et al., 2017), Autoregressive large language models (Llama, released by Meta AI), and Vision-transformer models for image recognition (Dosovitskiy et al., 2021). These models utilize deep learning principles, specifically neural networks, to analyze and generate human language or images. Their advancements lie in their capability to handle large-scale data and complex context through effective attention mechanisms and parallel processing.


# Question 2.a.ii: What is the marketing response for question 2?
test_question_2_a_ii: Some key architectures behind the development of large language models include transformers and self-attention mechanisms. Transformers, introduced by Vaswani et al. (2017), have been instrumental in developing large-scale language models due to their efficiency in handling long-range dependencies within sequential data. Self-attention mechanisms, used extensively in transformers, allow models to selectively focus on relevant parts of input data when making predictions, improving overall performance. Other important developments include the use of decoder-only architectures, which streamline processing and enhance text data generation. Additionally, techniques such as masked language modeling and fine-tuning on specific tasks or datasets help optimize the capabilities of these large language models.


# Question 2.b: What is the context you passed to the LLM for question 2?
test_question_2_b: \nA large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n\n\n== History ==\n\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets ("web as corpus"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\n\nA language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\n\n\n== Pure statistical models ==\n\n\n=== Models based on word n-grams ===\n\n\n=== Exponential ===\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\n\nthat meaningfully represents the people impacted by the technology, and that synthesizes peoples’\nvalues in a way that achieves broad consensus amongst many groups. We discuss some related\nconsiderations in Section 5.2.\n5.5\nBroader impacts\nThis work is motivated by our aim to increase the positive impact of large language models by training\nthem to do what a given set of humans want them to do. By default, language models optimize\nthe next word prediction objective, which is only a proxy for what we want these models to do.\nOur results indicate that our techniques hold promise for making language models more helpful,\ntruthful, and harmless. In the longer term, alignment failures could lead to more severe consequences,\nparticularly if these models are deployed in safety-critical situations. We expect that as model scaling\ncontinues, greater care has to be taken to ensure that they are aligned with human intentions (Bostrom,\n2014).\nHowever, making language models better at following user intentions also makes them easier to\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\nabusive content.\nAlignment techniques are not a panacea for resolving safety issues associated with large language\nmodels; rather, they should be used as one tool in a broader safety ecosystem. Aside from intentional\nmisuse, there are many domains where large language models should be deployed only with great\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\npeople based on protected characteristics, determining eligibility for credit, employment, or hous-\ning, generating political advertisements, and law enforcement. If these models are open-sourced,\nit becomes challenging to limit harmful applications in these and other domains without proper\nregulation. On the other hand, if large language model access is restricted to a few organizations\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\n\npeople based on protected characteristics, determining eligibility for credit, employment, or hous-\ning, generating political advertisements, and law enforcement. If these models are open-sourced,\nit becomes challenging to limit harmful applications in these and other domains without proper\nregulation. On the other hand, if large language model access is restricted to a few organizations\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\ntechnology. Another option is for an organization to own the end-to-end infrastructure of model\ndeployment, and make it accessible via an API. This allows for the implementation of safety protocols\nlike use case restriction (only allowing the model to be used for certain applications), monitoring\nfor misuse and revoking access to those who misuse the system, and rate limiting to prevent the\ngeneration of large-scale misinformation. However, this can come at the cost of reduced transparency\nand increased centralization of power because it requires the API provider to make decisions on\nwhere to draw the line on each of these questions.\nFinally, as discussed in Section 5.2, the question of who these models are aligned to is extremely\nimportant, and will signiﬁcantly affect whether the net impact of these models is positive or negative.\n20\n\nsystems. arXiv preprint arXiv:1905.00537.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWeidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M.,\nBalle, B., Kasirzadeh, A., et al. (2021). Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359.\nWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli,\nP., Coppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. arXiv\npreprint arXiv:2109.07445.\nWu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. (2021).\nRecursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.\nXu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein, D. (2021). Detoxifying language\nmodels risks marginalizing minority voices. arXiv preprint arXiv:2104.06390.\nXu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. (2020). Recipes for safety in\nopen-domain chatbots. arXiv preprint arXiv:2010.07079.\nYi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh, A., Gabriel, R., and\nHakkani-Tur, D. (2019). Towards coherent and engaging spoken dialog response generation\nusing automatic conversation evaluators. arXiv preprint arXiv:1904.13015.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). Hellaswag: Can a machine\nreally ﬁnish your sentence? In Association for Computational Linguistics, pages 4791–4800.\nZhao, M., Anderson, P., Jain, V., Wang, S., Ku, A., Baldridge, J., and Ie, E. (2021). On the evaluation\nof vision-and-language navigation instructions. arXiv preprint arXiv:2101.10504.\nZhou, W. and Xu, K. (2020). Learning to compare for better training and evaluation of open domain\nnatural language generation models. arXiv preprint arXiv:2002.05058.\n\nLlama (Large Language Model Meta AI, formerly stylized as LLaMA) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3.3, released in December 2024.\nModel weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 1B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n\n\n== Background ==\nAfter the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\nCompared with other responses to ChatGPT, Meta\'s Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\nAn empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the "Chinchilla-optimal" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022.\nFinetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369–2380. Association for Computational\nLinguistics.\nHamed Zamani, Gord Lueck, Everest Chen, Rodolfo\nQuispe, Flint Luu, and Nick Craswell. 2020. MIM-\nICS: A large-scale data collection for search clarifi-\ncation. In CIKM ’20: The 29th ACM International\nConference on Information and Knowledge Manage-\nment, Virtual Event, Ireland, October 19-23, 2020,\npages 3189–3196. ACM.\nChen Zhang, Dawei Song, Zheyu Ye, and Yan Gao.\n2023a. Towards the law of capacity gap in distilling\nlanguage models. CoRR, abs/2311.07052.\nLonghui Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, Meishan Zhang, and Min Zhang. 2023b.\nRankinggpt: Empowering large language models in\ntext ranking with progressive enhancement. CoRR,\nabs/2311.16720.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Zhicheng Dou, and\nJi-Rong Wen. 2023. Large language models for infor-\nmation retrieval: A survey. CoRR, abs/2308.07107.\nShengyao Zhuang, Bing Liu, Bevan Koopman, and\nGuido Zuccon. 2023. Open-source large language\nmodels are strong zero-shot query likelihood models\nfor document ranking. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2023,\nSingapore, December 6-10, 2023, pages 8807–8817.\nAssociation for Computational Linguistics.\nA\nDetails about Tasks & Datasets\nWe introduce the tasks and datasets as follows.\n\n1. Introduction\nLarge language models (LLMs) (Anthropic, 2023; Brown et al., 2020; Google, 2023; Jiang et al.,\n2023a; OpenAI, 2022, 2023; Radford et al., 2018, 2019; Gemini Team, 2023; Touvron et al., 2023a,b;\nYin et al., 2023; Zhao et al., 2023) based on Transformers (Vaswani et al., 2017) have become a\nprominent pathway to Artificial General Intelligence (AGI). LLMs acquire massive world knowledge\nby learning to predict the next word on large-scale web corpora. The capabilities of LLMs have been\ncontinuously increasing by scaling model sizes, dataset sizes, and computation. After pre-training,\nLLMs can be aligned to support real-world use cases by supervised fine-tuning (Chung et al., 2022;\nSanh et al., 2022) and preference optimization techniques including reinforcement learning from\nhuman feedback (RLHF) (Ouyang et al., 2022a; Wang et al., 2024; Zheng et al., 2023b) and direct\npreference optimization (DPO) (Rafailov et al., 2023). The capabilities of LLMs have empowered\nvarious applications including ChatGPT, Claude, Bard, Microsoft Copilot, Character.AI, Notion AI, etc.\nRecently, many specialized LLMs have been trained for different targeted usage scenarios. In general,\nLLMs specialize according to the targeted domains (e.g., finance (Wu et al., 2023), healthcare (Yang\net al., 2022b), legal (Cui et al., 2023), etc.) and tasks (e.g., role-playing (Wang et al., 2023d),\ncoding (Rozière et al., 2023), etc.). However, the ability of LLMs to write human-like texts and\nproduce creative content, which is a critical use case of LLM applications such as ChatGPT, is mostly\noverlooked by the community.\nIn this report, we focus on the literature domain and the task of writing, or content creation,\nand introduce Weaver, a family of LLMs dedicatedly pre-trained and aligned for this purpose. The\nname "Weaver" symbolizes the model’s proficiency in skillfully amalgamating linguistic elements,\nakin to the way a craftsman weaves threads to form a fabric. We answer four main questions in this\n\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024, The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\nAs of 2024, the largest and most capable models are all based on the Transformer architecture. Some recent implementations are based o\n\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger,\nG., Kim, J. W., Kreps, S., et al. (2019). Release strategies and the social impacts of language\nmodels. arXiv preprint arXiv:1908.09203.\nSolaiman, I. and Dennison, C. (2021). Process for adapting language models to society (palms) with\nvalues-targeted datasets. arXiv preprint arXiv:2106.10328.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D.,\nand Christiano, P. (2020). Learning to summarize from human feedback. arXiv preprint\narXiv:2009.01325.\nTamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021). Understanding the capabilities,\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\nT., Baker, L., Du, Y., et al. (2022). Lamda: Language models for dialog applications. arXiv\npreprint arXiv:2201.08239.\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. M. (2020).\nInvestigating gender bias in language models using causal mediation analysis. In NeurIPS.\nVölske, M., Potthast, M., Syed, S., and Stein, B. (2017). Tl; dr: Mining reddit to learn automatic\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages\n59–63.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman,\nS. R. (2019). Superglue: A stickier benchmark for general-purpose language understanding\nsystems. arXiv preprint arXiv:1905.00537.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWeidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M.,\nBalle, B., Kasirzadeh, A., et al. (2021). Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359.\n\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\n2002.08910.\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/1500000019.\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\nArXiv, abs/1908.09203, 2019.\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n14\n\nGuy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent\nSifre. An empirical analysis of compute-optimal large language model training. In Advances in\nNeural Information Processing Systems, volume 35, 2022.\n[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\nTriviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension.\narXiv preprint\narXiv:1705.03551, 2017.\n[16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, 7:453–466, 2019.\n8\n\n[29] K. Sun, Y. E. Xu, H. Zha, Y. Liu, and X. L. Dong. Head-to-Tail: How knowledgeable are\nlarge language models (llms)? a.k.a. will llms replace knowledge graphs?\narXiv preprint\narXiv:2308.10168, 2024.\n[30] Y. Sun, H. Xin, K. Sun, Y. E. Xu, X. Yang, X. L. Dong, N. Tang, and L. Chen. Are large\nlanguage models a good replacement of taxonomies? Proc. VLDB Endow., 17(11):2919–2932,\naug 2024.\n[31] A. Talmor and J. Berant. The web as a knowledge-base for answering complex questions, 2018.\n[32] N. Tang, C. Yang, J. Fan, L. Cao, Y. Luo, and A. Y. Halevy. Verifai: Verified generative AI. In\nCIDR, 2024.\n[33] Y. Tang and Y. Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-\nhop queries. arXiv preprint arXiv:2401.15391, 2024.\n[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[35] R. Usbeck, X. Yan, A. Perevalov, L. Jiang, J. Schulz, A. Kraft, C. Möller, J. Huang, J. Reineke,\nA.-C. Ngonga Ngomo, et al. QALD-10–the 10th challenge on question answering over linked\ndata. Semantic Web, (Preprint):1–15, 2023.\n[36] T. Vu, M. Iyyer, X. Wang, N. Constant, J. Wei, J. Wei, C. Tar, Y.-H. Sung, D. Zhou, Q. Le, and\nT. Luong. FreshLLMs: Refreshing large language models with search engine augmentation,\n2023.\n\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n\naugmented language model pre-training.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\nParanjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. 2023. Lost in the middle: How lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nOpenAI. 2023. Gpt-4 technical report.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, and Michael Bender-\nsky. 2023. Large language models are effective text\nrankers with pairwise ranking prompting.\narXiv\npreprint arXiv:2306.17563v1.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\n\n


# Question 2.c: List the doc_source for each of the documents in the context for question 2
test_question_2_c: [{'score': 0.8708409819341928,
  'source': 'https://en.wikipedia.org/wiki/Large_language_model'},
 {'score': 0.8503167852744612,
  'source': 'https://en.wikipedia.org/wiki/Language_model'},
 {'score': 0.8289898152449584,
  'source': 'https://arxiv.org/pdf/2005.11401.pdf'},
 {'score': 0.8227261033666393,
  'source': 'https://arxiv.org/pdf/2401.17268.pdf'},
 {'score': 0.8184026952898966,
  'source': 'https://arxiv.org/pdf/2106.09685.pdf'},
 {'score': 0.8139415448628228,
  'source': 'https://arxiv.org/pdf/2309.08872.pdf'},
 {'score': 0.8120952630616577,
  'source': 'https://arxiv.org/pdf/2401.17268.pdf'},
 {'score': 0.8106089346144765,
  'source': 'https://arxiv.org/pdf/2306.15595.pdf'},
 {'score': 0.8072513232085283,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'},
 {'score': 0.8066143983791149,
  'source': 'https://arxiv.org/pdf/2306.15595.pdf'},
 {'score': 0.8051586956026905,
  'source': 'https://arxiv.org/pdf/2306.15595.pdf'},
 {'score': 0.8048668467335733,
  'source': 'https://arxiv.org/pdf/2401.06532.pdf'},
 {'score': 0.8036303112274228,
  'source': 'https://arxiv.org/pdf/2310.11511.pdf'},
 {'score': 0.8028782847229095,
  'source': 'https://arxiv.org/pdf/2305.18290.pdf'},
 {'score': 0.8006807428819127,
  'source': 'https://en.wikipedia.org/wiki/Llama_(language_model)'}]


# Question 2.d.i: How well does your model perform relative to the gold answer we provided for engineering on question 2?
test_question_2_d_i: [('answer_relevance_score',4),
                  ('avg_chunk_score',0.8172668484270171),
                  ('comprehensive_score',3.5),
                  ('context_relevance_score',5),
                  ('failthfulness_score',1)]


# Question 2.d.ii:  How well does your model perform relative to the gold answer we provided for marketing on question 2?
test_question_2_d_ii: Paste list of [metric name,score tuples] here
                   [('answer_relevance_score', 5),
                   ('avg_chunk_score', 0.8172668484270171),
                   ('comprehensive_score', 4.25),
                   ('context_relevance_score', 5),
                   ('failthfulness_score', 4)]

# ------------------------------------------------------------------
# | Section (3): Test Question 3 (Question 83 in test set)    |
# ------------------------------------------------------------------


# Question 3.a.i: What is the engineering response for question 3?
test_question_3_a_i: absolutely! Here are some specific large language models and the companies or organizations that have developed them:\n\n1. **ChatGPT**: Developed by OpenAI\n2. **Claude** and **Bard**: Developed by Google\n3. **Microsoft Copilot**: Developed by Microsoft\n4. **Character.AI**: Developed by Character.ai\n5. **Notion AI**: Developed by Notion\n6. **Llama**, including versions 1, 2, and 3.2: Developed by Meta AI (Formerly known as Facebook AI Research)\n7. **Gemini Team**: Developed by the Gemini Team\n8. **Touvron et al.'s models**: Developed by Touvron, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman\n9. **Yin et al.'s models**: Developed by Yin, Zhao, Cui, and Zhao\n\nThese models have revolutionized the field of natural language processing and continue to shape the future of artificial intelligence.


# Question 3.a.ii: What is the marketing response for question 3?
test_question_3_a_ii: Absolutely! Here are some well-known large language models and the companies or organizations behind them:\n\n1. **ChatGPT**: Developed by OpenAI, a leading research institute and innovation lab focusing on artificial intelligence.\n2. **Claude**: From Anthropic, a research institute focused on developing advanced AI systems that can understand and interact with the world.\n3. **Microsoft Copilot**: Part of Microsoft's Azure Cognitive Services offering, providing intelligent assistance through conversational interfaces.\n4. **Character.AI**: A company creating AI characters for various purposes, including customer service representatives and entertainment.\n5. **Notion AI**: Integrated into Notion, a popular productivity app, for enhancing note-taking and documentation functionality.\n6. **LLama (Large Language Model Meta AI)**: Released by Meta AI, the artificial intelligence research branch of Meta Platforms Inc.\n\nThese models represent just a fraction of the advancements in large language models. Each organization brings unique perspectives and approaches to the development of these innovative technologies.


# Question 3.b: What is the context you passed to the LLM for question 3?
test_question_3_b: \nA large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n\n\n== History ==\n\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets ("web as corpus"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\n\nA language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\n\n\n== Pure statistical models ==\n\n\n=== Models based on word n-grams ===\n\n\n=== Exponential ===\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\n\nthat meaningfully represents the people impacted by the technology, and that synthesizes peoples’\nvalues in a way that achieves broad consensus amongst many groups. We discuss some related\nconsiderations in Section 5.2.\n5.5\nBroader impacts\nThis work is motivated by our aim to increase the positive impact of large language models by training\nthem to do what a given set of humans want them to do. By default, language models optimize\nthe next word prediction objective, which is only a proxy for what we want these models to do.\nOur results indicate that our techniques hold promise for making language models more helpful,\ntruthful, and harmless. In the longer term, alignment failures could lead to more severe consequences,\nparticularly if these models are deployed in safety-critical situations. We expect that as model scaling\ncontinues, greater care has to be taken to ensure that they are aligned with human intentions (Bostrom,\n2014).\nHowever, making language models better at following user intentions also makes them easier to\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\nabusive content.\nAlignment techniques are not a panacea for resolving safety issues associated with large language\nmodels; rather, they should be used as one tool in a broader safety ecosystem. Aside from intentional\nmisuse, there are many domains where large language models should be deployed only with great\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\npeople based on protected characteristics, determining eligibility for credit, employment, or hous-\ning, generating political advertisements, and law enforcement. If these models are open-sourced,\nit becomes challenging to limit harmful applications in these and other domains without proper\nregulation. On the other hand, if large language model access is restricted to a few organizations\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\n\npeople based on protected characteristics, determining eligibility for credit, employment, or hous-\ning, generating political advertisements, and law enforcement. If these models are open-sourced,\nit becomes challenging to limit harmful applications in these and other domains without proper\nregulation. On the other hand, if large language model access is restricted to a few organizations\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\ntechnology. Another option is for an organization to own the end-to-end infrastructure of model\ndeployment, and make it accessible via an API. This allows for the implementation of safety protocols\nlike use case restriction (only allowing the model to be used for certain applications), monitoring\nfor misuse and revoking access to those who misuse the system, and rate limiting to prevent the\ngeneration of large-scale misinformation. However, this can come at the cost of reduced transparency\nand increased centralization of power because it requires the API provider to make decisions on\nwhere to draw the line on each of these questions.\nFinally, as discussed in Section 5.2, the question of who these models are aligned to is extremely\nimportant, and will signiﬁcantly affect whether the net impact of these models is positive or negative.\n20\n\nsystems. arXiv preprint arXiv:1905.00537.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWeidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M.,\nBalle, B., Kasirzadeh, A., et al. (2021). Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359.\nWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli,\nP., Coppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. arXiv\npreprint arXiv:2109.07445.\nWu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. (2021).\nRecursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.\nXu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein, D. (2021). Detoxifying language\nmodels risks marginalizing minority voices. arXiv preprint arXiv:2104.06390.\nXu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. (2020). Recipes for safety in\nopen-domain chatbots. arXiv preprint arXiv:2010.07079.\nYi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh, A., Gabriel, R., and\nHakkani-Tur, D. (2019). Towards coherent and engaging spoken dialog response generation\nusing automatic conversation evaluators. arXiv preprint arXiv:1904.13015.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). Hellaswag: Can a machine\nreally ﬁnish your sentence? In Association for Computational Linguistics, pages 4791–4800.\nZhao, M., Anderson, P., Jain, V., Wang, S., Ku, A., Baldridge, J., and Ie, E. (2021). On the evaluation\nof vision-and-language navigation instructions. arXiv preprint arXiv:2101.10504.\nZhou, W. and Xu, K. (2020). Learning to compare for better training and evaluation of open domain\nnatural language generation models. arXiv preprint arXiv:2002.05058.\n\nLlama (Large Language Model Meta AI, formerly stylized as LLaMA) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3.3, released in December 2024.\nModel weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 1B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n\n\n== Background ==\nAfter the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\nCompared with other responses to ChatGPT, Meta\'s Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\nAn empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the "Chinchilla-optimal" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022.\nFinetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369–2380. Association for Computational\nLinguistics.\nHamed Zamani, Gord Lueck, Everest Chen, Rodolfo\nQuispe, Flint Luu, and Nick Craswell. 2020. MIM-\nICS: A large-scale data collection for search clarifi-\ncation. In CIKM ’20: The 29th ACM International\nConference on Information and Knowledge Manage-\nment, Virtual Event, Ireland, October 19-23, 2020,\npages 3189–3196. ACM.\nChen Zhang, Dawei Song, Zheyu Ye, and Yan Gao.\n2023a. Towards the law of capacity gap in distilling\nlanguage models. CoRR, abs/2311.07052.\nLonghui Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, Meishan Zhang, and Min Zhang. 2023b.\nRankinggpt: Empowering large language models in\ntext ranking with progressive enhancement. CoRR,\nabs/2311.16720.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Zhicheng Dou, and\nJi-Rong Wen. 2023. Large language models for infor-\nmation retrieval: A survey. CoRR, abs/2308.07107.\nShengyao Zhuang, Bing Liu, Bevan Koopman, and\nGuido Zuccon. 2023. Open-source large language\nmodels are strong zero-shot query likelihood models\nfor document ranking. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2023,\nSingapore, December 6-10, 2023, pages 8807–8817.\nAssociation for Computational Linguistics.\nA\nDetails about Tasks & Datasets\nWe introduce the tasks and datasets as follows.\n\n1. Introduction\nLarge language models (LLMs) (Anthropic, 2023; Brown et al., 2020; Google, 2023; Jiang et al.,\n2023a; OpenAI, 2022, 2023; Radford et al., 2018, 2019; Gemini Team, 2023; Touvron et al., 2023a,b;\nYin et al., 2023; Zhao et al., 2023) based on Transformers (Vaswani et al., 2017) have become a\nprominent pathway to Artificial General Intelligence (AGI). LLMs acquire massive world knowledge\nby learning to predict the next word on large-scale web corpora. The capabilities of LLMs have been\ncontinuously increasing by scaling model sizes, dataset sizes, and computation. After pre-training,\nLLMs can be aligned to support real-world use cases by supervised fine-tuning (Chung et al., 2022;\nSanh et al., 2022) and preference optimization techniques including reinforcement learning from\nhuman feedback (RLHF) (Ouyang et al., 2022a; Wang et al., 2024; Zheng et al., 2023b) and direct\npreference optimization (DPO) (Rafailov et al., 2023). The capabilities of LLMs have empowered\nvarious applications including ChatGPT, Claude, Bard, Microsoft Copilot, Character.AI, Notion AI, etc.\nRecently, many specialized LLMs have been trained for different targeted usage scenarios. In general,\nLLMs specialize according to the targeted domains (e.g., finance (Wu et al., 2023), healthcare (Yang\net al., 2022b), legal (Cui et al., 2023), etc.) and tasks (e.g., role-playing (Wang et al., 2023d),\ncoding (Rozière et al., 2023), etc.). However, the ability of LLMs to write human-like texts and\nproduce creative content, which is a critical use case of LLM applications such as ChatGPT, is mostly\noverlooked by the community.\nIn this report, we focus on the literature domain and the task of writing, or content creation,\nand introduce Weaver, a family of LLMs dedicatedly pre-trained and aligned for this purpose. The\nname "Weaver" symbolizes the model’s proficiency in skillfully amalgamating linguistic elements,\nakin to the way a craftsman weaves threads to form a fabric. We answer four main questions in this\n\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024, The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\nAs of 2024, the largest and most capable models are all based on the Transformer architecture. Some recent implementations are based o\n\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger,\nG., Kim, J. W., Kreps, S., et al. (2019). Release strategies and the social impacts of language\nmodels. arXiv preprint arXiv:1908.09203.\nSolaiman, I. and Dennison, C. (2021). Process for adapting language models to society (palms) with\nvalues-targeted datasets. arXiv preprint arXiv:2106.10328.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D.,\nand Christiano, P. (2020). Learning to summarize from human feedback. arXiv preprint\narXiv:2009.01325.\nTamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021). Understanding the capabilities,\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\nT., Baker, L., Du, Y., et al. (2022). Lamda: Language models for dialog applications. arXiv\npreprint arXiv:2201.08239.\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. M. (2020).\nInvestigating gender bias in language models using causal mediation analysis. In NeurIPS.\nVölske, M., Potthast, M., Syed, S., and Stein, B. (2017). Tl; dr: Mining reddit to learn automatic\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages\n59–63.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman,\nS. R. (2019). Superglue: A stickier benchmark for general-purpose language understanding\nsystems. arXiv preprint arXiv:1905.00537.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWeidinger, L., Mellor, J., Rauh, M., Grifﬁn, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M.,\nBalle, B., Kasirzadeh, A., et al. (2021). Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359.\n\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\n2002.08910.\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/1500000019.\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\nArXiv, abs/1908.09203, 2019.\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n14\n\nGuy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent\nSifre. An empirical analysis of compute-optimal large language model training. In Advances in\nNeural Information Processing Systems, volume 35, 2022.\n[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\nTriviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension.\narXiv preprint\narXiv:1705.03551, 2017.\n[16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, 7:453–466, 2019.\n8\n\n[29] K. Sun, Y. E. Xu, H. Zha, Y. Liu, and X. L. Dong. Head-to-Tail: How knowledgeable are\nlarge language models (llms)? a.k.a. will llms replace knowledge graphs?\narXiv preprint\narXiv:2308.10168, 2024.\n[30] Y. Sun, H. Xin, K. Sun, Y. E. Xu, X. Yang, X. L. Dong, N. Tang, and L. Chen. Are large\nlanguage models a good replacement of taxonomies? Proc. VLDB Endow., 17(11):2919–2932,\naug 2024.\n[31] A. Talmor and J. Berant. The web as a knowledge-base for answering complex questions, 2018.\n[32] N. Tang, C. Yang, J. Fan, L. Cao, Y. Luo, and A. Y. Halevy. Verifai: Verified generative AI. In\nCIDR, 2024.\n[33] Y. Tang and Y. Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-\nhop queries. arXiv preprint arXiv:2401.15391, 2024.\n[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[35] R. Usbeck, X. Yan, A. Perevalov, L. Jiang, J. Schulz, A. Kraft, C. Möller, J. Huang, J. Reineke,\nA.-C. Ngonga Ngomo, et al. QALD-10–the 10th challenge on question answering over linked\ndata. Semantic Web, (Preprint):1–15, 2023.\n[36] T. Vu, M. Iyyer, X. Wang, N. Constant, J. Wei, J. Wei, C. Tar, Y.-H. Sung, D. Zhou, Q. Le, and\nT. Luong. FreshLLMs: Refreshing large language models with search engine augmentation,\n2023.\n\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n\naugmented language model pre-training.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\nParanjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. 2023. Lost in the middle: How lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nOpenAI. 2023. Gpt-4 technical report.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, and Michael Bender-\nsky. 2023. Large language models are effective text\nrankers with pairwise ranking prompting.\narXiv\npreprint arXiv:2306.17563v1.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\n\n


# Question 3.c: List the doc_source for each of the documents in the context for question 3
test_question_3_c: [{'score': 0.8540815358899516,
  'source': 'https://en.wikipedia.org/wiki/Large_language_model'},
 {'score': 0.8307889064515293,
  'source': 'https://en.wikipedia.org/wiki/Language_model'},
 {'score': 0.8111769416663525,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'},
 {'score': 0.8107551779146172,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'},
 {'score': 0.8093750353915841,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'},
 {'score': 0.7989708986769997,
  'source': 'https://en.wikipedia.org/wiki/Llama_(language_model)'},
 {'score': 0.7949699688335639,
  'source': 'https://arxiv.org/pdf/2401.06532.pdf'},
 {'score': 0.7932349373665906,
  'source': 'https://arxiv.org/pdf/2401.17268.pdf'},
 {'score': 0.7892265112156589,
  'source': 'https://en.wikipedia.org/wiki/Large_language_model'},
 {'score': 0.7884422755291389,
  'source': 'https://arxiv.org/pdf/2203.02155.pdf'},
 {'score': 0.787253304876536, 'source': 'https://arxiv.org/pdf/2005.11401.pdf'},
 {'score': 0.7866016222640517,
  'source': 'https://arxiv.org/pdf/2310.06825.pdf'},
 {'score': 0.7860885874472796,
  'source': 'https://arxiv.org/pdf/2406.04744.pdf'},
 {'score': 0.7856137831798391,
  'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'},
 {'score': 0.7803296978095721,
  'source': 'https://arxiv.org/pdf/2312.05708.pdf'}]


# ------------------------------------------------------------------
# | Section (4): Compare and Contrast                              |
# ------------------------------------------------------------------


# Question 4.a: For run E questions 1 and 2, how well do you feel your metrics captured the differences and similarities between your answer and the gold answer?
compare_and_contrast_4_a: The generated answers are similar to the gold answers for both personas.  Question 1's has a high pairwise score of 4, in spite of the fact that the generated answer was much longer than the gold answer (the discrepency should have been penalized).  For Question 1, the pairwise score for both personas was >= 4.0 (in contrast to question 1's pairwise score of 1, which may in part be due to the longer length of the generated answer, since that was one criterion).  Both personas got fairly high comprehensive scores >= 3.5  Question 2 had high comprehensive scores >= 4.0 and pairwise scores >= 3.  The generated answer for Quesetion 2 engineering did discuss Transformer models such as Llama, but didn't discuss BERT.  


# ------------------------------------------------------------------
# | Section (5): Other Questions                                   |
# ------------------------------------------------------------------


# Question 5.3.a: How would you expect your response quality to change if you had a chunk size of 50?
other_questions_5_3_a: Larger chunk_sizes are consistently related to higher overall scores across all hyperparameters.  I would expect a chunk size of 50 to result in poor quality output.  It is also a function of other hyperparameters such as k_top.


# Question 5.3.b: How would you expect your response quality to change if you had a chunk size of 5000?
other_questions_5_3_b: Larger chunk_sizes are consistently related to higher overall scores across all hyperparameters.  A chunk size of 5000 was measured relative to other sizes and had consistently higher metric scores.  Generated text was more aligned across comprehensive score and pairwise score.


# Question 5.3.c: If you had time, how do you think fine-tuning of the LLM could help?  What type of data would you want for that? And which training approach would you take?
other_questions_5_3_c: It would be interesting to try to see what happens ---the gold dataset is 75 questions with textual density so I would guess it could lead to a measurement performance increase.  In principle, it should also be possible to train an LLM to augment the provided gold dataset with generated answers comparable to the gold answers.  This could then be used to fine-tune a judging reinforcement model.


# Question 5.3.d: What was your design philosophy of the prompts? How did they differ between engineering and marketing support?
other_questions_5_3_d: There were four judge prompt templates and a RAG prompt template.  The set of defined variables passed was: persona, persona_instructions, question, gold_answer, generated_answer, context, and length.  The judging criteria were largely the same for both personas but also had two variables for length.  Each persona had detailed specific instructions related to answer length, technical density, lists, and other requirements.


# Question 5.3.e: What are your average and peak load estimates for the system? Given that, would you suggest a pay-per-use deployment or one that reserves the LLM?
other_questions_5_3_e: Performance was steady, with all gen_model and judge_model combinations generating answers at a stable throughput.  The mean time per question, including printing and saving all data,  was about 6 mins/run, where each run was comprised of 15 questions.  Memory usage was stable.


# Question 5.3.f: What type of limitations/risks would you see in using this system?
other_questions_5_3_f: I was able to demonstrate repeatedly that specific instructions, used as a component of the RAG prompt template, were able to guarantee that Cohere would not hallucinate if information was missing from the context, but could not do the same for Mistral.  Hallucinations seemed under better control for both models, but they (along w/ associated risks to safety) arre always a concern.  In order to meet the true needs of both personas in terms of generated conetnt, one would certainly need to have formal objectives and dialects.
